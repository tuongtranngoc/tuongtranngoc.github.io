---
title: 'BERT: Combining the Best of Both Worlds'
date: 2024-01-21
permalink: /posts/2024/bert/
tags:
  - Mathematics
  - Natural Language Processing 
  - Deep Learning
---

<head>
    <style type="text/css">
        figure{text-align: center;}
        math{text-align: center;}
    </style>
</head>

# Introduction

### From context-independent to context-sensitive
For both word2vec and GlobVe assign the same pretrained vector to the same word regardless of the context of the word (if any)
Formally, a context-independent representation of any token $x$ is function $f(x)$ that only takes $x$ as its input.

For instance, the word **have** in context *I have a pencil* and *I have lunch* has completely different meanings; thus the same word may be assigned different representations depending on contexts.

Besides, a context-sensitive representation of token $x$ is a function $f(x, c(x))$ depending on both $x$ and its context $c(x)$.

For example, the previous apporoaches (ELMO) take the entire sequence as input and is a function that assign a representations to each word from the input sequence. However, the existing supervised model is specifically customized for a given task. Leveraging different best models for different tasks at that time.

### From Task-Specific to Task-Agnostic

Basically, ELMO still hings on a task-specific architecture. It is practically non-trivial to scraft a specific architecture for every natural language processing tasks.

GPT model represents an effort in designing a general task-agnostic model for context-sensitive representations. However, GPT only looks forward (left-to-right). For instance, a context-sensitive representation of token $x$ is a function $f(x, c_1(x), c_2(x))$, GPT will return the same representation for token $x$, though it has different meanings.