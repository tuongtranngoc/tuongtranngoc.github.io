---
title: 'SSD: Single Shot MultiBox Detector'
date: 2023-09-16
permalink: /posts/2023/ssd/
tags:
  - Computer Vision
  - Object Detection
  - Image Processing
---

In the object detection algorithm series, I will brifely give a high-level description of everything you need to know about the Pytorch's implementation of SSD algorithm as described on the [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) paper

## How does SSD works

SSD uses small convolutional filters applied to feature maps to predict category scores and box offsets for a fixed set of default boxes. To detect objects of different sizes, SSD outputs predictions from feature maps of different scales and explicitly separates predictions by aspect ratio.

During training, we need to first match the groundtruth to the default boxes and then using those matches to estimate the loss function. During inference time, similar prediction boxes are combined to estimate the final predictions.

<head>
    <style type="text/css">
        figure{text-align: center;}
        math{text-align: center;}
    </style>
</head>

<figure>
    <img src='/images/posts/ssd/detection_examples_coco.jpg'>
</figure>

## The Single Shot MultiBox Detector (SSD)
This section decribes the components of SSD architecture

### DefaultBoxes Generator
To handle difference object scales, SSD use both the lower and upper feature map for detection. Based on pre-defined feature sizes, the algorithm create a set of default boxes of different espect ratios at each location.

<figure>
    <img src='/images/posts/ssd/ssd_framework.jpg'>
</figure>

Assume that we want to use $m$ feature maps for detection, the scales of default boxes for each feature map is computed as:

$$s_k=s_{min} + \frac{s_{max}-s_{min}}{m-1} (k-1), k\in [1, m]$$

where $s_{min}=0.2$ and $s_{max}=0.9$, and different espect ratios for default boxes denote as $a_r=\lbrace 1, 2, 3, \frac{1}{2}, \frac{1}{3}\rbrace$. The width, height and center for each default box is computed as:

$$w_k=s_k\times\sqrt{a_r}, h_k=s_k/\sqrt{a_r}, cx_k=\frac{i+0.5}{\vert{f_k}\vert}, cy_k=\frac{j+0.5}{\vert{f_k}\vert}$$

where $\vert{f_k}\vert$ is the size of the k-th square feature map, $i,j \in [0, \vert{f_k}\vert)$. For the aspect ratio of 1, we also add a default box whose scale is $s'_k=\sqrt{s_{k} \times s_{k+1}}$ resulting in 6 default boxes per feature map location.

Specifically in the example below, the default boxes are generated by [DefaultboxesGenerator](https://github.com/tuongtranngoc/SSD/blob/main/ssd/data/default_boxes.py#L12) class :

<figure>
    <img src='/images/posts/ssd/dfboxes_generator.png'>
</figure>

### Matching default boxes
We first match each groundtruth box to the default boxes with the best jaccard overlap higher than a threshold (eg. 0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.

Example after applying [matching default boxes](https://github.com/tuongtranngoc/SSD/blob/main/ssd/data/voc.py#L36) with threshold=0.4:

<figure>
    <img src='/images/posts/ssd/matched_dfboxes.png'>
</figure>

### Feature Extractor VGG16

<figure>
    <img src='/images/posts/ssd/vgg16.png'>
</figure>

Each added feature layer can produce a fixed set of detection predictions using a set of convolutional filters: 
+ Feature map 38x38 with `conv4_3(FC6)`: $38 \times 38 \times 6 = 8664$
+ Feature map 19x19 with `conv7(FC7)`: $19 \times 19 \times 6 = 2166$
+ Feature map 10x10 with `conv8_2`: $10 \times 10 \times 6 = 600$
+ Feature map 5x5 with `conv9_2`: $5 \times 5 \times 6 = 150$
+ Feature map 3x3 with `conv10_2`: $3 \times 3 \times 6 = 54$
+ Feature map 1x1 with `conv11_2`: $1 \times 1 \times 6 = 6$

Total of default boxes: $8664+2166+600+150+54+6=11640$

Here are a few notable points in [Feature Extractor VGG16]() implementation:
+ Patching the [ceil_mode parameter]() of the 3rd maxpool layer is necessary to get the same feature map size 38x38.
+ Change maxpool5 from `2x2-s2` to `3x3-s1` and the use [Atrous algorithm]().
+ [L2 normalization]() is used on the output of `conv4_3` to scale the feature norm at each location in the feature map to 20 and learn the scale during back propagation.